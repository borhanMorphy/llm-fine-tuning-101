# ğŸ§  Educational Repository for Fine-Tuning Large Language Models (LLMs) â€” PyTorch Only

Welcome to the **LLM Fine-Tuning Educational Repository**, built entirely with **pure PyTorch** â€” no HuggingFace, no external frameworks. This project is designed to provide a hands-on, from-scratch learning experience for understanding and fine-tuning large language models. Perfect for learners who want to build a solid foundation by implementing everything step by step.

---

## ğŸ“š What You'll Learn

- Core architecture and internals of LLMs and SLMs
- Dataset handling, preprocessing and multi-gpu training with PyTorch
- Building tokenizers and vocabularies from scratch
- Implementing training loops and loss functions
- Fine-tuning techniques without relying on external libraries
- LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning
- DPO (Direct Preference Optimization) for aligning models using preference data

---

## ğŸ› ï¸ Repository Structure

```
.
â”œâ”€â”€ demo/                 # Markdowns for demos
â”œâ”€â”€ scripts/              # Utility scripts
â”œâ”€â”€ slm_full_sft.py       # Supervised Full Fine-Tuning Smollm2
â”œâ”€â”€ slm_lora_sft.py       # Supervised Fine-Tuning Smollm2 via LoRA
â”œâ”€â”€ llm_lora_sft.py       # Supervised Fine-Tuning Gemma3 via LoRA
â””â”€â”€ README.md             # This file
```

## ğŸ§ª Demos
| Markdown                           | Description                             |
| ---------------------------------- | --------------------------------------- |
| `YugiohGPT.md`                     | Yugioh Card generation via LLM          |


## ğŸ’¡ Why No HuggingFace?
This repo is intended for educational purposes. By not using external libraries, youâ€™ll:

- Learn how everything works under the hood
- Gain deep insight into training dynamics and model architecture
- Build skills that translate to research and custom implementations
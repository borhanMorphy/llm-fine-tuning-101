# Main Tasks
- [x] Understand RoPE
    - [x] Math-wise
    - [x] Code-wise
- [x] Implement RoPE
- [x] Start Implementing Decoder Layer
    - [x] MHA with RoPE
    - [x] MLP
- [x] Create a config file for Smollm2
- [x] Port weights and sanity check
- [x] Port tokenizer vocab and update chat template
- [x] Run tokenizer on the full dataset for sanity
- [x] Setup loss function and test backprop
- [x] Seperate train, val and test
- [x] Test validation loop and test loop
- [x] Manage Artifacts
    - [x] Model weights `.ckpt` file
    - [x] Tokenizer `.json` file
    - [x] Dataset .jsonl file
- [x] Upload dataset to hf (private)
- [x] Port code to lightning and ignite!
- [x] Upload to github
- [x] Clean the code
- [x] Fix the data
- [x] Support multi-gpu
- [ ] Retrained the model after fixing the data
- [ ] Export to vLLM offline (managed kv cache?)
- [x] Understand LoRAÃ§
    - [x] Math-wise
    - [x] Code-wise
- [x] Implement LoRA
- [x] LoRA Training
- [x] Understand DPO
    - [x] Math-wise
    - [x] Code-wise
- [x] Implement Gemma3 1b
    - [x] Port tokenizer vocab and update chat template
    - [x] Implement sliding window attention via flex attention
    - [x] Sanity check of whole implementation
    - [x] Weight porting script and sanity check
- [x] Switch from DP -> DDP
- [ ] Training with Gemma3 1b LoRA SFT
- [ ] Create a dataset for DPO
- [ ] Implement DPO
- [ ] Traing with DPO
- [ ] Training with Gemma3 1b LoRA DPO
- [ ] Training with Smollm2 1b LoRA DPO

# Known Issues
- [x] Fix mutli-gpu training for LoRA
